{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6e0b6395",
   "metadata": {},
   "source": [
    "## Tokenization with NLTK\n",
    "\n",
    "- Conversion of corpus into words, sentences and etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c7ecd1db",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Tokenization is a component of text preprocessing - (Data/Text Cleaning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1a445847",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9f328290",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\qc\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "69e8f09e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shahzaib is a veterinarian, he has a deep bond with animals. He is also getting expertise on understanding \n",
      "animal behaviour and welfare. He is also getting expertise during his internship at Microinformatics under the supervision of\n",
      "Haseeb Raza.\n"
     ]
    }
   ],
   "source": [
    "## Corpus\n",
    "## Document\n",
    "## Words\n",
    "## Vocabulary\n",
    "\n",
    "\n",
    "\n",
    "corpus = \"\"\"Shahzaib is a veterinarian, he has a deep bond with animals. He is also getting expertise on understanding \n",
    "animal behaviour and welfare. He is also getting expertise during his internship at Microinformatics under the supervision of\n",
    "Haseeb Raza.\"\"\"\n",
    "\n",
    "print(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "192cb64d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Shahzaib is a veterinarian, he has a deep bond with animals.', 'He is also getting expertise on understanding \\nanimal behaviour and welfare.', 'He is also getting expertise during his internship at Microinformatics under the supervision of\\nHaseeb Raza.']\n"
     ]
    }
   ],
   "source": [
    "## Practical of Tokenization\n",
    "## Tokenization ---> Corpus/Sentences ---> Sentences/Words\n",
    "\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "sentence = sent_tokenize(corpus)\n",
    "print(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "61fe506e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shahzaib is a veterinarian, he has a deep bond with animals.\n",
      "He is also getting expertise on understanding \n",
      "animal behaviour and welfare.\n",
      "He is also getting expertise during his internship at Microinformatics under the supervision of\n",
      "Haseeb Raza.\n"
     ]
    }
   ],
   "source": [
    "for sentence in sentence:\n",
    "    print(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a8abdbc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shahzaib\n",
      "is\n",
      "a\n",
      "veterinarian\n",
      ",\n",
      "he\n",
      "has\n",
      "a\n",
      "deep\n",
      "bond\n",
      "with\n",
      "animals\n",
      ".\n",
      "He\n",
      "is\n",
      "also\n",
      "getting\n",
      "expertise\n",
      "on\n",
      "understanding\n",
      "animal\n",
      "behaviour\n",
      "and\n",
      "welfare\n",
      ".\n",
      "He\n",
      "is\n",
      "also\n",
      "getting\n",
      "expertise\n",
      "during\n",
      "his\n",
      "internship\n",
      "at\n",
      "Microinformatics\n",
      "under\n",
      "the\n",
      "supervision\n",
      "of\n",
      "Haseeb\n",
      "Raza\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "words = word_tokenize(corpus)\n",
    "for word in words:\n",
    "    print(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "67e31660",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'method'>\n",
      "Shahzaib\n",
      "is\n",
      "a\n",
      "veterinarian\n",
      ",\n",
      "he\n",
      "has\n",
      "a\n",
      "deep\n",
      "bond\n",
      "with\n",
      "animals\n",
      ".\n",
      "He\n",
      "is\n",
      "also\n",
      "getting\n",
      "expertise\n",
      "on\n",
      "understanding\n",
      "animal\n",
      "behaviour\n",
      "and\n",
      "welfare\n",
      ".\n",
      "He\n",
      "is\n",
      "also\n",
      "getting\n",
      "expertise\n",
      "during\n",
      "his\n",
      "internship\n",
      "at\n",
      "Microinformatics\n",
      "under\n",
      "the\n",
      "supervision\n",
      "of\n",
      "Haseeb\n",
      "Raza\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import wordpunct_tokenize\n",
    "print(type(wordpunct_tokenize))\n",
    "\n",
    "words = wordpunct_tokenize(corpus)\n",
    "\n",
    "for word in words:\n",
    "    print(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "499b1e9c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['CONTRACTIONS2',\n",
       " 'CONTRACTIONS3',\n",
       " 'CONVERT_PARENTHESES',\n",
       " 'DOUBLE_DASHES',\n",
       " 'ENDING_QUOTES',\n",
       " 'PARENS_BRACKETS',\n",
       " 'PUNCTUATION',\n",
       " 'STARTING_QUOTES',\n",
       " '__abstractmethods__',\n",
       " '__class__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__getstate__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__sizeof__',\n",
       " '__slots__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " '_abc_impl',\n",
       " '_contractions',\n",
       " 'detokenize',\n",
       " 'span_tokenize',\n",
       " 'span_tokenize_sents',\n",
       " 'tokenize',\n",
       " 'tokenize_sents']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import TreebankWordDetokenizer\n",
    "\n",
    "tokenizer = TreebankWordDetokenizer()\n",
    "\n",
    "dir(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dc083a66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S h a h z a i b   i s   a   v e t e r i n a r i a n,   h e   h a s   a   d e e p   b o n d   w i t h   a n i m a l s .   H e   i s   a l s o   g e t t i n g   e x p e r t i s e   o n   u n d e r s t a n d i n g   \n",
      " a n i m a l   b e h a v i o u r   a n d   w e l f a r e .   H e   i s   a l s o   g e t t i n g   e x p e r t i s e   d u r i n g   h i s   i n t e r n s h i p   a t   M i c r o i n f o r m a t i c s   u n d e r   t h e   s u p e r v i s i o n   o f \n",
      " H a s e e b   R a z a.\n"
     ]
    }
   ],
   "source": [
    "words = tokenizer.tokenize(corpus)\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "67b7715e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S   h   a   h   z   a   i   b       i   s       a       v   e   t   e   r   i   n   a   r   i   a   n,       h   e       h   a   s       a       d   e   e   p       b   o   n   d       w   i   t   h       a   n   i   m   a   l   s   .       H   e       i   s       a   l   s   o       g   e   t   t   i   n   g       e   x   p   e   r   t   i   s   e       o   n       u   n   d   e   r   s   t   a   n   d   i   n   g       \n",
      "   a   n   i   m   a   l       b   e   h   a   v   i   o   u   r       a   n   d       w   e   l   f   a   r   e   .       H   e       i   s       a   l   s   o       g   e   t   t   i   n   g       e   x   p   e   r   t   i   s   e       d   u   r   i   n   g       h   i   s       i   n   t   e   r   n   s   h   i   p       a   t       M   i   c   r   o   i   n   f   o   r   m   a   t   i   c   s       u   n   d   e   r       t   h   e       s   u   p   e   r   v   i   s   i   o   n       o   f   \n",
      "   H   a   s   e   e   b       R   a   z   a.\n"
     ]
    }
   ],
   "source": [
    "corpus_2 = tokenizer.detokenize(words)\n",
    "\n",
    "print(corpus_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b7039048",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'abc.ABCMeta'>\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import TabTokenizer\n",
    "\n",
    "print(type(TabTokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "aea36152",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['__abstractmethods__',\n",
       " '__class__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__getstate__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__sizeof__',\n",
       " '__slots__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " '_abc_impl',\n",
       " '_string',\n",
       " 'span_tokenize',\n",
       " 'span_tokenize_sents',\n",
       " 'tokenize',\n",
       " 'tokenize_sents']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer_2 = TabTokenizer()\n",
    "\n",
    "dir(tokenizer_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9ef720e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shahzaib is a veterinarian, he has a deep bond with animals. He is also getting expertise on understanding \n",
      "animal behaviour and welfare. He is also getting expertise during his internship at Microinformatics under the supervision of\n",
      "Haseeb Raza.\n"
     ]
    }
   ],
   "source": [
    "words = tokenizer_2.tokenize(corpus)\n",
    "\n",
    "for word in words:\n",
    "    print(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c028dd91",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
